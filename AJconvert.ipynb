{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import glob\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10\n",
    "\n",
    "mdf=pd.read_excel(f'/workspaces/vikrant_dubai/RAW/AJ/AJ {n}.xlsx')\n",
    "excel_file = pd.ExcelFile(f'/workspaces/vikrant_dubai/RAW/AJ/AJ {n}.xlsx')\n",
    "\n",
    "df = pd.DataFrame(columns=[\"Station Name\", \"Latitude\", \"Longitude\", \"Region\", \"Area Classification\", \"Station Operation Year\"])\n",
    "dfs = []\n",
    "for i in range(len(excel_file.sheet_names)):\n",
    "\n",
    "    df = pd.read_excel(f\"/workspaces/vikrant_dubai/RAW/AJ/AJ {n}.xlsx\", sheet_name = excel_file.sheet_names[i])\n",
    "\n",
    "    df=df.drop([0,1,2,3,4,5])\n",
    "    df=df.iloc[:,:8]\n",
    "    df.head(2)\n",
    "\n",
    "    mapped_data = {\n",
    "        \"Station Name\": df.iloc[0,1],  \n",
    "        \"Latitude\": df.iloc[1, 2],  \n",
    "        \"Longitude\": df.iloc[1,4],  \n",
    "        \"Region\": df.iloc[0, 4],  \n",
    "        \"Area Classification\": df.iloc[0, 7], \n",
    "        \"Station Operation Year\": df.iloc[1, 7]   \n",
    "    }\n",
    "    temp_df = pd.DataFrame([mapped_data])\n",
    "    dfs.append(temp_df)\n",
    "\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "# final_df.to_csv(f'/workspaces/vikrant_dubai/final/details{k}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mon in range(1, 13):\n",
    "    n=mon \n",
    "\n",
    "    mdf=pd.read_excel(f'/workspaces/vikrant_dubai/RAW/AJ/AJ {n}.xlsx')\n",
    "    excel_file = pd.ExcelFile(f'/workspaces/vikrant_dubai/RAW/AJ/AJ {n}.xlsx')\n",
    "\n",
    "    columns_of_main_df = ['Station Name','Date', 'Time', 'SO2', 'Data Cap. (SO2)', 'SO2 Avg', 'NO2', 'Data Cap. (NO2)', 'NO2 Avg',      'O3', 'Data Cap. (O3)', 'O3 8H Avg.', 'Daily Capture O3', 'O3 Daily Average', 'O3 Daily Average (NS)', 'CO',  'Data Cap. (CO)', 'CO 8H Avg.', 'Daily Capture CO', 'CO Daily Average', 'PM10',    'Data Cap. (PM10)', 'PM10 Avg', 'PM2.5', 'Data Cap. (PM2.5)', 'PM2.5 Avg', 'Lower Ambient Temperature', 'Upper Ambient Temperature', 'Barometric Pressure', 'Relative Humidity', 'Wind Direction', 'Wind Speed', 'H2S', 'Toluene', 'O-Xylene', 'Ethylbenzene', 'MP-Xylene', 'Benzene', 'CH4', 'NMHC', 'THC', 'Noise', 'Solar Radiation','Atm P.']\n",
    "    for s in range(len(excel_file.sheet_names)):\n",
    "        main_df = pd.DataFrame(columns=columns_of_main_df)\n",
    "\n",
    "        # Read the Excel file and preprocess the data\n",
    "        mdf = pd.read_excel(f'/workspaces/vikrant_dubai/RAW/AJ/AJ {n}.xlsx', sheet_name=excel_file.sheet_names[s])\n",
    "        mdf = mdf.drop([0, 1, 2, 3,4,5,6,7,8,9]).reset_index(drop=True)\n",
    "        mdf.columns = mdf.iloc[0]\n",
    "        mdf = mdf[1:].reset_index(drop=True).drop([0, 1]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "        # Replace invalid 'Date/Time' entries with NaN\n",
    "        mdf['Date/Time'] = pd.to_datetime(mdf['Date/Time'], format='%d/%m/%Y %H:%M', errors='coerce')\n",
    "\n",
    "        # Convert 'Date/Time' column to separate 'Date' and 'Time' columns\n",
    "        mdf['Date/Time'] = pd.to_datetime(mdf['Date/Time'], format='%d/%m/%Y %H:%M', dayfirst=True)\n",
    "        mdf['Date'] = mdf['Date/Time'].dt.date\n",
    "        mdf['Time'] = mdf['Date/Time'].dt.time\n",
    "        new_df_columns = mdf.columns.tolist()\n",
    "\n",
    "        # Iterate over the series and replace the column names\n",
    "        for col_index in range(1, len(new_df_columns)):\n",
    "            if 'DATA CAP.' in new_df_columns[col_index]:\n",
    "                if new_df_columns[col_index-1] == 'DATA COUNT':\n",
    "                    new_df_columns[col_index] = f\"DATA CAP. ({new_df_columns[col_index-2]})\"\n",
    "                else:\n",
    "                    new_df_columns[col_index] = f\"DATA CAP. ({new_df_columns[col_index-1]})\"\n",
    "\n",
    "        mdf.columns = new_df_columns\n",
    "\n",
    "        # Reorder columns and drop the original 'Date/Time' column\n",
    "        mdf = mdf[['Date', 'Time'] + [col for col in mdf.columns if col not in ['Date', 'Time']]]\n",
    "        mdf = mdf.drop(columns=['Date/Time'])\n",
    "\n",
    "        # add column station name\n",
    "        mdf['Station Name'] = excel_file.sheet_names[s]\n",
    "\n",
    "        # Format the 'Date' column\n",
    "        mdf['Date'] = pd.to_datetime(mdf['Date']).dt.strftime('%d/%m/%Y')\n",
    "\n",
    "        # List of values to find matches for\n",
    "        values = [col.upper() for col in mdf.columns]\n",
    "\n",
    "        # List of column names\n",
    "        l1 = [col.upper() for col in main_df.columns]\n",
    "\n",
    "        # Loop through values and find the closest match\n",
    "        matches = {}\n",
    "        for value in values:\n",
    "            closest_match = difflib.get_close_matches(value, l1, n=1)\n",
    "            if closest_match:\n",
    "                main_df[main_df.columns[main_df.columns.str.upper() == closest_match[0]][0]] = mdf[mdf.columns[mdf.columns.str.upper() == value][0]]\n",
    "            else:\n",
    "                main_df[value] = ''\n",
    "\n",
    "\n",
    "            main_df.to_csv(f'/workspaces/vikrant_dubai/Transformed/{excel_file.sheet_names[s]}.csv', index=False)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "            main_df.to_csv(f'/workspaces/vikrant_dubai/Transformed/{excel_file.sheet_names[s]}.csv', index=False)\n",
    "        \n",
    "\n",
    "    # Get a list of all CSV files in the transformed folder\n",
    "    csv_files = [f for f in os.listdir('/workspaces/vikrant_dubai/Transformed') if f.endswith('.csv')]\n",
    "\n",
    "    # Read and concatenate all CSV files\n",
    "    combined_df = pd.concat([pd.read_csv(os.path.join('/workspaces/vikrant_dubai/Transformed', f)) for f in csv_files], ignore_index=True)\n",
    "\n",
    "    # Save the combined DataFrame to a new CSV file\n",
    "    combined_df.to_csv(f'/workspaces/vikrant_dubai/final/AJ/AJ {n}.csv', index=False)\n",
    "\n",
    "\n",
    "    # Get a list of all files in the Transformed folder\n",
    "    files = glob.glob('/workspaces/vikrant_dubai/Transformed/*')\n",
    "\n",
    "    # Iterate over the list of files and remove each one\n",
    "    for f in files:\n",
    "        os.remove(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all CSV files in the final/AJ folder\n",
    "final_aj_csv_files = [f for f in os.listdir('/workspaces/vikrant_dubai/final/AJ') if f.endswith('.csv')]\n",
    "\n",
    "# Read and concatenate all CSV files\n",
    "combined_final_aj_df = pd.concat([pd.read_csv(os.path.join('/workspaces/vikrant_dubai/final/AJ', f)) for f in final_aj_csv_files], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_final_aj_df.to_csv('/workspaces/vikrant_dubai/final/comb/AJ_combined.csv', index=False)\n",
    "\n",
    "# Remove all files in the final/AJ folder\n",
    "for f in final_aj_csv_files:\n",
    "    os.remove(os.path.join('/workspaces/vikrant_dubai/final/AJ', f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
